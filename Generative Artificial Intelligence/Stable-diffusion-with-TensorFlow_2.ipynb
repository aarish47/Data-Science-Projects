{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **High-performance Image Generation using Stable Diffusion in KerasCV**\n",
    "\n",
    "## **Written by:** [Aarish Asif Khan](https://www.kaggle.com/aarishasifkhan)\n",
    "\n",
    "##  **Date:** 27th March 2024\n",
    "\n",
    "## **Website of Tensorflow:** [Tensorflow Org](https://www.tensorflow.org/tutorials/generative/generate_images_with_stable_diffusion)\n",
    "\n",
    "## **Credits to:** fchollet, lukewood and divamgupta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Note:`**\n",
    "\n",
    "If you have no idea what Stable diffusion is, than check out my Previous notebook that I published, so you can understand the basic concepts required! Thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Overview**\n",
    "\n",
    "In this notebook, we will show how to generate novel images based on a text prompt using the` KerasCV implementation` of `stability.ai's` text-to-image model, `Stable Diffusion.`\n",
    "\n",
    "`Stable Diffusion is a powerful, open-source text-to-image generation model.` While there exist multiple open-source implementations that allow you to easily create images from textual prompts, KerasCV's offers a few distinct advantages. These include XLA compilation and mixed precision support, which together achieve state-of-the-art generation speed.\n",
    "\n",
    "To get started, let's install a few dependencies and sort out some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow keras_cv --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import keras_cv\n",
    "\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "Check out the power of `keras_cv.models.StableDiffusion().`\n",
    "\n",
    "First, we construct a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
     ]
    }
   ],
   "source": [
    "model = keras_cv.models.StableDiffusion(img_width=512, img_height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will give the model a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n",
      "\u001b[1m1356917/1356917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "WARNING:tensorflow:From c:\\Users\\nawaz\\miniconda3\\envs\\tf_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:174: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\n",
      "\u001b[1m492466864/492466864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 0us/step\n",
      "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5\n",
      "\u001b[1m3439090152/3439090152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1510s\u001b[0m 0us/step\n",
      "\u001b[1m 6/50\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55:06\u001b[0m 75s/step"
     ]
    }
   ],
   "source": [
    "images = model.text_to_image(\"photograph of an astronaut riding a horse\", batch_size=3)\n",
    "\n",
    "\n",
    "def plot_images(images):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(images):\n",
    "    keras.preprocessing.image.save_img(f\"image_{i}.png\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's not all this model can do, let's try a more complex prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.text_to_image(\n",
    "    \"cute magical flying dog, fantasy art, \"\n",
    "    \"golden color, high quality, highly detailed, elegant, sharp focus, \"\n",
    "    \"concept art, character concepts, digital painting, mystery, adventure\",\n",
    "    batch_size=3,\n",
    ")\n",
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How are these Images generated?**\n",
    "\n",
    "Unlike what you might expect at this point, StableDiffusion doesn't actually run on magic. It's a kind of `\"latent diffusion model\"`. Let's dig into what that means.\n",
    "\n",
    "You may be familiar with the idea of super-resolution: it's possible to train a deep learning model to denoise an input image -- and thereby turn it into a higher-resolution version. The deep learning model doesn't do this by magically recovering the information that's missing from the noisy, low-resolution input -- rather, the model uses its training data distribution to hallucinate the visual details that would be most likely given the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Perks of KerasCV**\n",
    "\n",
    "With several implementations of Stable Diffusion publicly available why should you use `keras_cv.models.StableDiffusion?`\n",
    "\n",
    "Aside from the easy-to-use API, KerasCV's Stable Diffusion model comes with some powerful advantages, including:\n",
    "\n",
    "* **`Graph mode execution`**\n",
    "\n",
    "* **`XLA compilation through jit_compile=True`**\n",
    "\n",
    "* **`Support for mixed precision computation`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_result = []\n",
    "start = time.time()\n",
    "images = model.text_to_image(\n",
    "    \"A cute otter in a rainbow whirlpool holding shells, watercolor\",\n",
    "    batch_size=3,\n",
    ")\n",
    "end = time.time()\n",
    "benchmark_result.append([\"Standard\", end - start])\n",
    "plot_images(images)\n",
    "\n",
    "print(f\"Standard model: {(end - start):.2f} seconds\")\n",
    "keras.backend.clear_session()  # Clear session to preserve memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mixed Precision**\n",
    "\n",
    "`\"Mixed precision\"` consists of performing computation using float16 precision, while storing weights in the float32 format. This is done to take advantage of the fact that float16 operations are backed by significantly faster kernels than their float32 counterparts on modern NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_cv.models.StableDiffusion()\n",
    "\n",
    "print(\"Compute dtype:\", model.diffusion_model.compute_dtype)\n",
    "print(\n",
    "    \"Variable dtype:\",\n",
    "    model.diffusion_model.variable_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm up model to run graph tracing before benchmarking.\n",
    "model.text_to_image(\"warming up the model\", batch_size=3)\n",
    "\n",
    "start = time.time()\n",
    "images = model.text_to_image(\n",
    "    \"a cute magical flying dog, fantasy art, \"\n",
    "    \"golden color, high quality, highly detailed, elegant, sharp focus, \"\n",
    "    \"concept art, character concepts, digital painting, mystery, adventure\",\n",
    "    batch_size=3,\n",
    ")\n",
    "end = time.time()\n",
    "benchmark_result.append([\"Mixed Precision\", end - start])\n",
    "plot_images(images)\n",
    "\n",
    "print(f\"Mixed precision model: {(end - start):.2f} seconds\")\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XLA Compilation**\n",
    "\n",
    "TensorFlow comes with the `XLA: Accelerated Linear Algebra compiler built-in keras_cv.models.`StableDiffusion supports a jit_compile argument out of the box. Setting this argument to True enables XLA compilation, resulting in a significant speed-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set back to the default for benchmarking purposes.\n",
    "keras.mixed_precision.set_global_policy(\"float32\")\n",
    "\n",
    "model = keras_cv.models.StableDiffusion(jit_compile=True)\n",
    "\n",
    "# Before we benchmark the model, we run inference once to make sure the TensorFlow\n",
    "images = model.text_to_image(\"An avocado armchair\", batch_size=3)\n",
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "images = model.text_to_image(\n",
    "    \"A cute otter in a rainbow whirlpool holding shells, watercolor\",\n",
    "    batch_size=3,\n",
    ")\n",
    "end = time.time()\n",
    "benchmark_result.append([\"XLA\", end - start])\n",
    "plot_images(images)\n",
    "\n",
    "print(f\"With XLA: {(end - start):.2f} seconds\")\n",
    "keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
